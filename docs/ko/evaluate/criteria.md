# 평가 기준

<div class="language-support-tag">
    <span class="lst-supported">ADK에서 지원</span><span class="lst-python">Python</span>
</div>

이 페이지에서는 도구 사용 궤적, 응답 품질 및 안전성을 포함하여 에이전트 성능을 평가하기 위해 ADK에서 제공하는 평가 기준을 간략하게 설명합니다.

기준 | 설명 | 참조 기반 | 루브릭 필요 | LLM-as-a-Judge | [사용자 시뮬레이션](./user-sim.md) 지원
:--------------------------------------- | :-------------------------------------------------------- | :-------------- | :--------------- | :------------- | :----------------------------------------
`tool_trajectory_avg_score` | 도구 호출 궤적의 정확한 일치 | 예 | 아니요 | 아니요 | 아니요
`response_match_score` | 참조 응답에 대한 ROUGE-1 유사성 | 예 | 아니요 | 아니요 | 아니요
`final_response_match_v2` | 참조 응답에 대한 LLM 판단 의미론적 일치 | 예 | 아니요 | 예 | 아니요
`rubric_based_final_response_quality_v1` | 사용자 지정 루브릭을 기반으로 한 LLM 판단 최종 응답 품질 | 아니요 | 예 | 예 | 아니요
`rubric_based_tool_use_quality_v1` | 사용자 지정 루브릭을 기반으로 한 LLM 판단 도구 사용 품질 | 아니요 | 예 | 예 | 아니요
`hallucinations_v1` | 컨텍스트에 대한 에이전트 응답의 LLM 판단 근거 | 아니요 | 아니요 | 예 | 예
`safety_v1` | 에이전트 응답의 안전성/무해성 | 아니요 | 아니요 | 예 | 예

## tool_trajectory_avg_score

이 기준은 에이전트가 호출한 도구의 시퀀스를 예상 호출 목록과 비교하고 `EXACT`, `IN_ORDER` 또는 `ANY_ORDER` 중 하나의 일치 유형을 기반으로 평균 점수를 계산합니다.

#### 이 기준을 언제 사용해야 합니까?

이 기준은 에이전트 정확성이 도구 호출에 따라 달라지는 시나리오에 이상적입니다. 도구 호출을 얼마나 엄격하게 따라야 하는지에 따라 `EXACT`, `IN_ORDER` 및 `ANY_ORDER`의 세 가지 일치 유형 중 하나를 선택할 수 있습니다.

이 메트릭은 특히 다음에 유용합니다.

*   **회귀 테스트:** 에이전트 업데이트가 설정된 테스트 사례에 대한 도구 호출 동작을 의도치 않게 변경하지 않도록 합니다.
*   **워크플로 유효성 검사:** 에이전트가 특정 순서로 특정 API 호출이 필요한 미리 정의된 워크플로를 올바르게 따르는지 확인합니다.
*   **고정밀 작업:** 도구 매개변수 또는 호출 순서의 약간의 편차가 상당히 다르거나 잘못된 결과를 초래할 수 있는 작업을 평가합니다.

특정 도구 실행 경로를 적용하고 도구 이름, 인수 또는 순서의 편차를 실패로 간주해야 하는 경우 `EXACT` 일치를 사용합니다.

특정 주요 도구 호출이 특정 순서로 발생하는지 확인하고 싶지만 그 사이에 다른 도구 호출이 발생하는 것을 허용하는 경우 `IN_ORDER` 일치를 사용합니다. 이 옵션은 특정 주요 작업 또는 도구 호출이 특정 순서로 발생하는지 확인하고 다른 도구 호출도 발생할 여지를 남겨두는 데 유용합니다.

특정 주요 도구 호출이 발생하는지 확인하고 싶지만 순서는 신경 쓰지 않고 그 사이에 다른 도구 호출이 발생하는 것을 허용하는 경우 `ANY_ORDER` 일치를 사용합니다. 이 기준은 에이전트가 5개의 검색 쿼리를 실행하는 것과 같이 동일한 개념에 대한 여러 도구 호출이 발생하는 경우에 유용합니다. 검색 쿼리가 발생하는 순서는 신경 쓰지 않고 발생하기만 하면 됩니다.

#### 세부 정보

평가 중인 각 호출에 대해 이 기준은 에이전트가 생성한 도구 호출 목록을 세 가지 일치 유형 중 하나를 사용하여 예상 도구 호출 목록과 비교합니다. 선택한 일치 유형을 기반으로 도구 호출이 일치하면 해당 호출에 대해 1.0의 점수가 부여되고 그렇지 않으면 점수는 0.0입니다. 최종 값은 평가 사례의 모든 호출에 대한 이러한 점수의 평균입니다.

비교는 다음 일치 유형 중 하나를 사용하여 수행할 수 있습니다.

*   **`EXACT`**: 추가 또는 누락된 도구 호출 없이 실제 및 예상 도구 호출 간의 완벽한 일치가 필요합니다.
*   **`IN_ORDER`**: 예상 목록의 모든 도구 호출이 실제 목록에 동일한 순서로 있어야 하지만 그 사이에 다른 도구 호출이 나타날 수 있습니다.
*   **`ANY_ORDER`**: 예상 목록의 모든 도구 호출이 실제 목록에 어떤 순서로든 있어야 하며 그 사이에 다른 도구 호출이 나타날 수 있습니다.

#### 이 기준을 사용하는 방법은 무엇입니까?

기본적으로 `tool_trajectory_avg_score`는 `EXACT` 일치 유형을 사용합니다. `EvalConfig`의 `criteria` 사전에서 `EXACT` 일치 유형에 대한 이 기준의 임계값만 지정할 수 있습니다. 값은 0.0에서 1.0 사이의 부동 소수점이어야 하며, 이는 평가 사례가 통과하기 위한 최소 허용 점수를 나타냅니다. 모든 호출에서 도구 궤적이 정확히 일치할 것으로 예상되는 경우 임계값을 1.0으로 설정해야 합니다.

`EXACT` 일치에 대한 `EvalConfig` 항목 예:

```json
{
  "criteria": {
    "tool_trajectory_avg_score": 1.0
  }
}
```

또는 `match_type`을 명시적으로 지정할 수 있습니다.

```json
{
  "criteria": {
    "tool_trajectory_avg_score": {
      "threshold": 1.0,
      "match_type": "EXACT"
    }
  }
}
```


`IN_ORDER` 또는 `ANY_ORDER` 일치 유형을 사용하려면 임계값과 함께 `match_type` 필드를 통해 지정할 수 있습니다.

`IN_ORDER` 일치에 대한 `EvalConfig` 항목 예:

```json
{
  "criteria": {
    "tool_trajectory_avg_score": {
      "threshold": 1.0,
      "match_type": "IN_ORDER"
    }
  }
}
```

`ANY_ORDER` 일치에 대한 `EvalConfig` 항목 예:

```json
{
  "criteria": {
    "tool_trajectory_avg_score": {
      "threshold": 1.0,
      "match_type": "ANY_ORDER"
    }
  }
}
```

#### 출력 및 해석 방법

출력은 0.0에서 1.0 사이의 점수이며, 1.0은 모든 호출에 대해 실제 및 예상 도구 궤적 간의 완벽한 일치를 나타내고 0.0은 모든 호출에 대한 완전한 불일치를 나타냅니다. 점수가 높을수록 좋습니다. 1.0 미만의 점수는 적어도 하나의 호출에 대해 에이전트의 도구 호출 궤적이 예상 궤적에서 벗어났음을 의미합니다.

## response_match_score

이 기준은 에이전트의 최종 응답이 Rouge-1을 사용하여 황금/예상 최종 응답과 일치하는지 평가합니다.

### 이 기준을 언제 사용해야 합니까?

에이전트의 출력이 콘텐츠 중복 측면에서 예상 출력과 얼마나 밀접하게 일치하는지에 대한 정량적 측정이 필요한 경우 이 기준을 사용합니다.

### 세부 정보

ROUGE-1은 시스템 생성 텍스트(후보 요약)와 참조 텍스트 간의 유니그램(단일 단어) 중복을 구체적으로 측정합니다. 기본적으로 참조 텍스트의 개별 단어 중 몇 개가 후보 텍스트에 있는지 확인합니다. 자세한 내용은 [ROUGE-1](https://github.com/google-research/google-research/tree/master/rouge)에 대한 세부 정보를 참조하십시오.

### 이 기준을 사용하는 방법은 무엇입니까?

`EvalConfig`의 `criteria` 사전에서 이 기준의 임계값을 지정할 수 있습니다. 값은 0.0에서 1.0 사이의 부동 소수점이어야 하며, 이는 평가 사례가 통과하기 위한 최소 허용 점수를 나타냅니다.

`EvalConfig` 항목 예:

```json
{
  "criteria": {
    "response_match_score": 0.8
  }
}
```

### 출력 및 해석 방법

이 기준의 값 범위는 [0,1]이며 1에 가까울수록 바람직합니다.

## final_response_match_v2

이 기준은 에이전트의 최종 응답이 LLM을 심사관으로 사용하여 황금/예상 최종 응답과 일치하는지 평가합니다.

### 이 기준을 언제 사용해야 합니까?

에이전트의 최종 응답의 정확성을 참조와 비교하여 평가해야 하지만 답변이 제시되는 방식에 유연성이 필요한 경우 이 기준을 사용합니다. 핵심 의미와 정보가 참조와 일치하는 한 다른 표현이나 형식이 허용되는 경우에 적합합니다. 이 기준은 의미론적 동등성이 정확한 어휘 중복보다 더 중요한 질문 답변, 요약 또는 기타 생성 작업을 평가하는 데 좋은 선택이며 `response_match_score`보다 더 정교한 대안입니다.

### 세부 정보

이 기준은 대규모 언어 모델(LLM)을 심사관으로 사용하여 에이전트의 최종 응답이 제공된 참조 응답과 의미론적으로 동등한지 여부를 결정합니다. 에이전트의 응답에 올바른 정보가 포함되어 있는지에 초점을 맞추고 서식, 표현 또는 추가적인 올바른 세부 정보 포함의 차이를 허용하므로 어휘 일치 메트릭(`response_match_score` 등)보다 유연하도록 설계되었습니다.

각 호출에 대해 이 기준은 심사관 LLM에 에이전트의 응답을 참조와 비교하여 "유효" 또는 "무효"로 평가하도록 프롬프트합니다. 이는 견고성을 위해 여러 번 반복되며(구성 가능한 `num_samples`를 통해) 과반수 투표로 호출이 1.0(유효) 또는 0.0(무효)의 점수를 받는지 여부를 결정합니다. 최종 기준 점수는 전체 평가 사례에서 유효하다고 간주되는 호출의 비율입니다.

### 이 기준을 사용하는 방법은 무엇입니까?

이 기준은 `LlmAsAJudgeCriterion`을 사용하므로 평가 임계값, 심사관 모델 및 호출당 샘플 수를 구성할 수 있습니다.

`EvalConfig` 항목 예:

```json
{
  "criteria": {
    "final_response_match_v2": {
      "threshold": 0.8,
      "judge_model_options": {
            "judge_model": "gemini-1.5-flash",
            "num_samples": 5
          }
        }
    }
  }
}
```

### 출력 및 해석 방법

이 기준은 0.0에서 1.0 사이의 점수를 반환합니다. 1.0의 점수는 LLM 심사관이 모든 호출에 대해 에이전트의 최종 응답을 유효하다고 간주했음을 의미하며, 0.0에 가까운 점수는 많은 응답이 참조 응답과 비교할 때 무효하다고 판단되었음을 나타냅니다. 값이 높을수록 좋습니다.

## rubric_based_final_response_quality_v1

이 기준은 LLM을 심사관으로 사용하여 사용자 정의 루브릭 세트에 대해 에이전트의 최종 응답 품질을 평가합니다.

### 이 기준을 언제 사용해야 합니까?

단순한 정확성이나 참조와의 의미론적 동등성을 넘어서는 응답 품질 측면을 평가해야 하는 경우 이 기준을 사용합니다. 어조, 스타일, 유용성 또는 루브릭에 정의된 특정 대화 지침 준수와 같은 미묘한 속성을 평가하는 데 이상적입니다. 이 기준은 단일 참조 응답이 없거나 품질이 여러 주관적인 요인에 따라 달라지는 경우에 특히 유용합니다.

### 세부 정보

이 기준은 루브릭으로 정의한 특정 기준에 따라 응답 품질을 평가하는 유연한 방법을 제공합니다. 예를 들어 응답이 간결한지, 사용자 의도를 올바르게 추론하는지 또는 전문 용어를 피하는지 확인하는 루브릭을 정의할 수 있습니다.

이 기준은 LLM-as-a-judge를 사용하여 각 루브릭에 대해 에이전트의 최종 응답을 평가하고 각 루브릭에 대해 `yes`(1.0) 또는 `no`(0.0) 판정을 내립니다. 다른 LLM 기반 메트릭과 마찬가지로 호출당 심사관 모델을 여러 번 샘플링하고 과반수 투표를 사용하여 해당 호출의 각 루브릭에 대한 점수를 결정합니다. 호출에 대한 전체 점수는 루브릭 점수의 평균입니다. 평가 사례에 대한 최종 기준 점수는 모든 호출에 대한 이러한 전체 점수의 평균입니다.

### 이 기준을 사용하는 방법은 무엇입니까?

이 기준은 `RubricsBasedCriterion`을 사용하며, `EvalConfig`에 루브릭 목록을 제공해야 합니다. 각 루브릭은 고유한 ID와 내용으로 정의되어야 합니다.

`EvalConfig` 항목 예:

```json
{
  "criteria": {
    "rubric_based_final_response_quality_v1": {
      "threshold": 0.8,
      "judge_model_options": {
        "judge_model": "gemini-1.5-flash",
        "num_samples": 5
      },
      "rubrics": [
        {
          "rubric_id": "conciseness",
          "rubric_content": {
            "text_property": "에이전트의 응답은 직접적이고 요점에 맞습니다."
          }
        },
        {
          "rubric_id": "intent_inference",
          "rubric_content": {
            "text_property": "에이전트의 응답은 모호한 쿼리에서 사용자의 기본 목표를 정확하게 추론합니다."
          }
        }
      ]
    }
  }
}
```

### 출력 및 해석 방법

이 기준은 0.0에서 1.0 사이의 전체 점수를 출력하며, 1.0은 에이전트의 응답이 모든 호출에서 모든 루브릭을 충족했음을 나타내고 0.0은 충족된 루브릭이 없음을 나타냅니다. 결과에는 각 호출에 대한 자세한 루브릭별 점수도 포함됩니다. 값이 높을수록 좋습니다.

## rubric_based_tool_use_quality_v1

이 기준은 LLM을 심사관으로 사용하여 사용자 정의 루브릭 세트에 대해 에이전트의 도구 사용 품질을 평가합니다.

### 이 기준을 언제 사용해야 합니까?

최종 응답이 올바른지 여부뿐만 아니라 에이전트가 도구를 *어떻게* 사용하는지 평가해야 하는 경우 이 기준을 사용합니다. 에이전트가 올바른 도구를 선택했는지, 올바른 매개변수를 사용했는지 또는 특정 순서의 도구 호출을 따랐는지 평가하는 데 이상적입니다. 이는 에이전트 추론 프로세스를 검증하고, 도구 사용 오류를 디버깅하고, 특히 여러 도구 사용 경로가 유사한 최종 답변으로 이어질 수 있지만 하나의 경로만 올바른 것으로 간주되는 경우 규정된 워크플로 준수를 보장하는 데 유용합니다.

### 세부 정보

이 기준은 루브릭으로 정의한 특정 규칙에 따라 도구 사용을 평가하는 유연한 방법을 제공합니다. 예를 들어 특정 도구가 호출되었는지, 매개변수가 올바른지 또는 도구가 특정 순서로 호출되었는지 확인하는 루브릭을 정의할 수 있습니다.

이 기준은 LLM-as-a-judge를 사용하여 각 루브릭에 대해 에이전트의 도구 호출 및 응답을 평가하고 각 루브릭에 대해 `yes`(1.0) 또는 `no`(0.0) 판정을 내립니다. 다른 LLM 기반 메트릭과 마찬가지로 호출당 심사관 모델을 여러 번 샘플링하고 과반수 투표를 사용하여 해당 호출의 각 루브릭에 대한 점수를 결정합니다. 호출에 대한 전체 점수는 루브릭 점수의 평균입니다. 평가 사례에 대한 최종 기준 점수는 모든 호출에 대한 이러한 전체 점수의 평균입니다.

### 이 기준을 사용하는 방법은 무엇입니까?

이 기준은 `RubricsBasedCriterion`을 사용하며, `EvalConfig`에 루브릭 목록을 제공해야 합니다. 각 루브릭은 고유한 ID와 내용으로 정의되어야 하며 평가할 도구 사용의 특정 측면을 설명해야 합니다.

`EvalConfig` 항목 예:

```json
{
  "criteria": {
    "rubric_based_tool_use_quality_v1": {
      "threshold": 1.0,
      "judge_model_options": {
        "judge_model": "gemini-1.5-flash",
        "num_samples": 5
      },
      "rubrics": [
        {
          "rubric_id": "geocoding_called",
          "rubric_content": {
            "text_property": "에이전트는 GetWeather 도구를 호출하기 전에 GeoCoding 도구를 호출합니다."
          }
        },
        {
          "rubric_id": "getweather_called",
          "rubric_content": {
            "text_property": "에이전트는 사용자의 위치에서 파생된 좌표로 GetWeather 도구를 호출합니다."
          }
        }
      ]
    }
  }
}
```

### 출력 및 해석 방법

이 기준은 0.0에서 1.0 사이의 전체 점수를 출력하며, 1.0은 에이전트의 도구 사용이 모든 호출에서 모든 루브릭을 충족했음을 나타내고 0.0은 충족된 루브릭이 없음을 나타냅니다. 결과에는 각 호출에 대한 자세한 루브릭별 점수도 포함됩니다. 값이 높을수록 좋습니다.

## hallucinations_v1

이 기준은 모델 응답에 거짓, 모순 또는 지원되지 않는 주장이 포함되어 있는지 평가합니다.

### 이 기준을 언제 사용해야 합니까?

에이전트의 응답이 제공된 컨텍스트(예: 도구 출력, 사용자 쿼리, 지침)에 근거하고 환각을 포함하지 않도록 하려면 이 기준을 사용합니다.

### 세부 정보

이 기준은 개발자 지침, 사용자 프롬프트, 도구 정의, 도구 호출 및 그 결과를 포함하는 컨텍스트를 기반으로 모델 응답에 거짓, 모순 또는 지원되지 않는 주장이 포함되어 있는지 평가합니다. LLM-as-a-judge를 사용하며 두 단계 프로세스를 따릅니다.

1.  **세그먼터**: 에이전트 응답을 개별 문장으로 분할합니다.
2.  **문장 유효성 검사기**: 근거를 위해 제공된 컨텍스트에 대해 각 분할된 문장을 평가합니다. 각 문장은 `supported`, `unsupported`, `contradictory`, `disputed` 또는 `not_applicable`으로 레이블이 지정됩니다.

메트릭은 정확도 점수를 계산합니다. `supported` 또는 `not_applicable`인 문장의 백분율입니다. 기본적으로 최종 응답만 평가됩니다. 기준에서 `evaluate_intermediate_nl_responses`가 true로 설정된 경우 에이전트의 중간 자연어 응답도 평가됩니다.

### 이 기준을 사용하는 방법은 무엇입니까?

이 기준은 `HallucinationsCriterion`을 사용하므로 평가 임계값, 심사관 모델, 호출당 샘플 수 및 중간 자연어 응답 평가 여부를 구성할 수 있습니다.

`EvalConfig` 항목 예:

```json
{
  "criteria": {
    "hallucinations_v1": {
      "threshold": 0.8,
      "judge_model_options": {
            "judge_model": "gemini-1.5-flash",
          },
      "evaluate_intermediate_nl_responses": true
    }
  }
}

```

### 출력 및 해석 방법

이 기준은 0.0에서 1.0 사이의 점수를 반환합니다. 1.0의 점수는 에이전트의 응답에 있는 모든 문장이 컨텍스트에 근거함을 의미하며, 0.0에 가까운 점수는 많은 문장이 거짓, 모순 또는 지원되지 않음을 나타냅니다. 값이 높을수록 좋습니다.

## safety_v1

이 기준은 에이전트 응답의 안전성(무해성)을 평가합니다.

### 이 기준을 언제 사용해야 합니까?

에이전트 응답이 안전 지침을 준수하고 유해하거나 부적절한 콘텐츠를 생성하지 않도록 해야 하는 경우 이 기준을 사용해야 합니다. 사용자 대면 애플리케이션 또는 응답 안전이 우선인 모든 시스템에 필수적입니다.

### 세부 정보

이 기준은 에이전트의 응답에 증오심 표현, 괴롭힘 또는 위험한 정보와 같은 유해한 콘텐츠가 포함되어 있는지 평가합니다. ADK 내에서 기본적으로 구현된 다른 메트릭과 달리 `safety_v1`은 평가를 Vertex AI General AI Eval SDK에 위임합니다.

### 이 기준을 사용하는 방법은 무엇입니까?

이 기준을 사용하려면 Google Cloud 프로젝트가 필요합니다. Vertex AI SDK가 올바르게 작동하려면 일반적으로 에이전트 디렉토리의 `.env` 파일에 `GOOGLE_CLOUD_PROJECT` 및 `GOOGLE_CLOUD_LOCATION` 환경 변수가 설정되어 있어야 합니다.

`EvalConfig`의 `criteria` 사전에서 이 기준의 임계값을 지정할 수 있습니다. 값은 0.0에서 1.0 사이의 부동 소수점이어야 하며, 응답이 통과로 간주되기 위한 최소 안전 점수를 나타냅니다.

`EvalConfig` 항목 예:

```json
{
  "criteria": {
    "safety_v1": 0.8
  }
}
```

### 출력 및 해석 방법

이 기준은 0.0에서 1.0 사이의 점수를 반환합니다. 1.0에 가까운 점수는 응답이 안전함을 나타내고 0.0에 가까운 점수는 잠재적인 안전 문제를 나타냅니다.