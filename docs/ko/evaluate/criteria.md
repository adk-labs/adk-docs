# 평가 기준

<div class="language-support-tag">
    <span class="lst-supported">ADK에서 지원</span><span class="lst-python">Python</span>
</div>

이 페이지에서는 도구 사용 궤적, 응답 품질 및 안전성을 포함하여 에이전트 성능을 평가하기 위해 ADK에서 제공하는 평가 기준을 간략하게 설명합니다.

기준                                | 설명                                               | 참조 기반 | 루브릭 필요 | LLM 판사 | [사용자 시뮬레이션](./user-sim.md) 지원
:--------------------------------------- | :-------------------------------------------------------- | :-------------- | :--------------- | :------------- | :----------------------------------------
`tool_trajectory_avg_score`              | 도구 호출 궤적의 정확한 일치                       | 예             | 아니요               | 아니요             | 아니요
`response_match_score`                   | 참조 응답에 대한 ROUGE-1 유사성                  | 예             | 아니요               | 아니요             | 아니요
`final_response_match_v2`                | 참조 응답에 대한 LLM 판단 의미 일치           | 예             | 아니요               | 예            | 아니요
`rubric_based_final_response_quality_v1` | 사용자 지정 루브릭에 기반한 LLM 판단 최종 응답 품질 | 아니요              | 예              | 예            | 아니요
`rubric_based_tool_use_quality_v1`       | 사용자 지정 루브릭에 기반한 LLM 판단 도구 사용 품질     | 아니요              | 예              | 예            | 아니요
`hallucinations_v1`                      | 컨텍스트에 대한 에이전트 응답의 LLM 판단 근거성 | 아니요              | 아니요               | 예            | 예
`safety_v1`                              | 에이전트 응답의 안전성/무해성                     | 아니요              | 아니요               | 예            | 예

## tool_trajectory_avg_score

이 기준은 에이전트가 호출한 도구 시퀀스를 예상 호출 목록과 비교하고 정확한 일치를 기반으로 평균 점수를 계산합니다.

### 이 기준을 사용하는 경우

이 기준은 에이전트의 동작 정확성이 정확한 인수와 함께 정확한 도구 호출 시퀀스를 따르는 데 엄격하게 의존하는 시나리오에 이상적입니다. 특정 도구 실행 경로를 적용해야 하고 도구 이름, 인수 또는 순서의 편차를 실패로 간주해야 하는 경우에 사용합니다. 특히 다음과 같은 경우에 유용합니다.

*   **회귀 테스트:** 에이전트 업데이트가 설정된 테스트 사례에 대한 도구 호출 동작을 의도치 않게 변경하지 않도록 보장합니다.
*   **워크플로 유효성 검사:** 에이전트가 특정 순서로 특정 API 호출이 필요한 미리 정의된 워크플로를 올바르게 따르는지 확인합니다.
*   **고정밀 작업:** 도구 매개변수나 호출 순서의 약간의 편차가 상당히 다르거나 잘못된 결과를 초래할 수 있는 작업을 평가합니다.

### 세부 정보

평가 중인 각 호출에 대해 이 기준은 에이전트가 생성한 도구 호출 목록과 순서를 예상 도구 호출 목록과 비교합니다. 비교는 목록의 각 도구 호출에 대한 도구 이름과 도구 인수에 대한 정확한 일치를 수행하여 수행됩니다. 호출의 모든 도구 호출이 내용과 순서에서 정확하게 일치하면 해당 호출에 대해 1.0의 점수가 부여되고 그렇지 않으면 점수는 0.0입니다. 최종 값은 평가 사례의 모든 호출에 대한 이러한 점수의 평균입니다.

### 이 기준을 사용하는 방법

`EvalConfig`의 `criteria` 사전에서 이 기준에 대한 임계값을 지정할 수 있습니다. 값은 0.0에서 1.0 사이의 부동 소수점이어야 하며, 이는 평가 사례가 통과하기 위한 최소 허용 점수를 나타냅니다. 모든 호출에서 도구 궤적이 정확하게 일치할 것으로 예상되는 경우 임계값을 1.0으로 설정해야 합니다.

`EvalConfig` 항목 예:

```json
{
  "criteria": {
    "tool_trajectory_avg_score": 1.0
  }
}
```

### 출력 및 해석 방법

출력은 0.0에서 1.0 사이의 점수이며, 1.0은 모든 호출에 대해 실제 및 예상 도구 궤적 간의 완벽한 일치를 나타내고 0.0은 모든 호출에 대한 완전한 불일치를 나타냅니다. 점수가 높을수록 좋습니다. 1.0 미만의 점수는 적어도 하나의 호출에 대해 에이전트의 도구 호출 궤적이 예상 궤적에서 벗어났음을 의미합니다.

## response_match_score

이 기준은 에이전트의 최종 응답이 Rouge-1을 사용하여 황금/예상 최종 응답과 일치하는지 평가합니다.

### 이 기준을 사용하는 경우

에이전트의 출력이 내용 중복 측면에서 예상 출력과 얼마나 밀접하게 일치하는지에 대한 정량적 측정이 필요할 때 이 기준을 사용합니다.

### 세부 정보

ROUGE-1은 시스템 생성 텍스트(후보 요약)와 참조 텍스트 간의 유니그램(단일 단어) 중복을 구체적으로 측정합니다. 기본적으로 참조 텍스트의 개별 단어 중 몇 개가 후보 텍스트에 있는지 확인합니다. 자세한 내용은 [ROUGE-1](https://github.com/google-research/google-research/tree/master/rouge)에 대한 세부 정보를 참조하십시오.

### 이 기준을 사용하는 방법

`EvalConfig`의 `criteria` 사전에서 이 기준에 대한 임계값을 지정할 수 있습니다. 값은 0.0에서 1.0 사이의 부동 소수점이어야 하며, 이는 평가 사례가 통과하기 위한 최소 허용 점수를 나타냅니다.

`EvalConfig` 항목 예:

```json
{
  "criteria": {
    "response_match_score": 0.8
  }
}
```

### 출력 및 해석 방법

이 기준의 값 범위는 [0,1]이며 1에 가까울수록 바람직합니다.

## final_response_match_v2

이 기준은 LLM을 판사로 사용하여 에이전트의 최종 응답이 황금/예상 최종 응답과 일치하는지 평가합니다.

### 이 기준을 사용하는 경우

에이전트의 최종 응답의 정확성을 참조와 비교하여 평가해야 하지만 답변이 제시되는 방식에 유연성이 필요한 경우 이 기준을 사용합니다. 핵심 의미와 정보가 참조와 일치하는 한 다른 표현이나 형식이 허용되는 경우에 적합합니다. 이 기준은 정확한 어휘 중복보다 의미적 동등성이 더 중요한 질문 답변, 요약 또는 기타 생성 작업을 평가하는 데 좋은 선택이므로 `response_match_score`보다 더 정교한 대안입니다.

### 세부 정보

이 기준은 대규모 언어 모델(LLM)을 판사로 사용하여 에이전트의 최종 응답이 제공된 참조 응답과 의미적으로 동등한지 확인합니다. 에이전트의 응답에 올바른 정보가 포함되어 있는지에 초점을 맞추고 서식, 표현 또는 추가적인 올바른 세부 정보 포함의 차이를 허용하므로 어휘 일치 메트릭(`response_match_score` 등)보다 더 유연하도록 설계되었습니다.

각 호출에 대해 이 기준은 판사 LLM에게 에이전트의 응답을 참조와 비교하여 "유효" 또는 "유효하지 않음"으로 평가하도록 요청합니다. 이는 견고성을 위해 여러 번 반복되며(`num_samples`를 통해 구성 가능) 다수결 투표를 통해 호출이 1.0(유효) 또는 0.0(유효하지 않음)의 점수를 받는지 결정합니다. 최종 기준 점수는 전체 평가 사례에서 유효하다고 간주되는 호출의 비율입니다.

### 이 기준을 사용하는 방법

이 기준은 `LlmAsAJudgeCriterion`을 사용하므로 평가 임계값, 판사 모델 및 호출당 샘플 수를 구성할 수 있습니다.

`EvalConfig` 항목 예:

```json
{
  "criteria": {
    "final_response_match_v2": {
      "threshold": 0.8,
      "judge_model_options": {
            "judge_model": "gemini-2.5-flash",
            "num_samples": 5
          }
        }
    }
  }
}
```

### 출력 및 해석 방법

이 기준은 0.0에서 1.0 사이의 점수를 반환합니다. 1.0의 점수는 LLM 판사가 모든 호출에 대해 에이전트의 최종 응답을 유효하다고 간주했음을 의미하며, 0.0에 가까운 점수는 많은 응답이 참조 응답과 비교할 때 유효하지 않다고 판단되었음을 나타냅니다. 값이 높을수록 좋습니다.

## rubric_based_final_response_quality_v1

이 기준은 LLM을 판사로 사용하여 사용자 정의 루브릭 세트에 대해 에이전트의 최종 응답 품질을 평가합니다.

### 이 기준을 사용하는 경우

단순한 정확성이나 참조와의 의미적 동등성을 넘어서는 응답 품질의 측면을 평가해야 하는 경우 이 기준을 사용합니다. 어조, 스타일, 유용성 또는 루브릭에 정의된 특정 대화 지침 준수와 같은 미묘한 속성을 평가하는 데 이상적입니다. 이 기준은 단일 참조 응답이 없거나 품질이 여러 주관적인 요인에 따라 달라지는 경우에 특히 유용합니다.

### 세부 정보

이 기준은 루브릭으로 정의한 특정 기준에 따라 응답 품질을 평가하는 유연한 방법을 제공합니다. 예를 들어 응답이 간결한지, 사용자 의도를 올바르게 추론하는지 또는 전문 용어를 피하는지 확인하는 루브릭을 정의할 수 있습니다.

이 기준은 LLM 판사를 사용하여 각 루브릭에 대해 에이전트의 최종 응답을 평가하고 각각에 대해 `예`(1.0) 또는 `아니요`(0.0) 판정을 내립니다. 다른 LLM 기반 메트릭과 마찬가지로 호출당 판사 모델을 여러 번 샘플링하고 다수결 투표를 사용하여 해당 호출의 각 루브릭에 대한 점수를 결정합니다. 호출의 전체 점수는 루브릭 점수의 평균입니다. 평가 사례의 최종 기준 점수는 모든 호출에 대한 이러한 전체 점수의 평균입니다.

### 이 기준을 사용하는 방법

이 기준은 `RubricsBasedCriterion`을 사용하며, `EvalConfig`에 루브릭 목록을 제공해야 합니다. 각 루브릭은 고유 ID와 해당 내용으로 정의되어야 합니다.

`EvalConfig` 항목 예:

```json
{
  "criteria": {
    "rubric_based_final_response_quality_v1": {
      "threshold": 0.8,
      "judge_model_options": {
        "judge_model": "gemini-2.5-flash",
        "num_samples": 5
      },
      "rubrics": [
        {
          "rubric_id": "conciseness",
          "rubric_content": {
            "text_property": "에이전트의 응답은 직접적이고 요점에 맞습니다."
          }
        },
        {
          "rubric_id": "intent_inference",
          "rubric_content": {
            "text_property": "에이전트의 응답은 모호한 쿼리에서 사용자의 기본 목표를 정확하게 추론합니다."
          }
        }
      ]
    }
  }
}
```

### 출력 및 해석 방법

이 기준은 0.0에서 1.0 사이의 전체 점수를 출력하며, 1.0은 에이전트의 응답이 모든 호출에서 모든 루브릭을 충족했음을 나타내고 0.0은 충족된 루브릭이 없음을 나타냅니다. 결과에는 각 호출에 대한 자세한 루브릭별 점수도 포함됩니다. 값이 높을수록 좋습니다.

## rubric_based_tool_use_quality_v1

이 기준은 LLM을 판사로 사용하여 사용자 정의 루브릭 세트에 대해 에이전트의 도구 사용 품질을 평가합니다.

### 이 기준을 사용하는 경우

최종 응답이 올바른지 여부뿐만 아니라 에이전트가 도구를 *어떻게* 사용하는지 평가해야 하는 경우 이 기준을 사용합니다. 에이전트가 올바른 도구를 선택했는지, 올바른 매개변수를 사용했는지 또는 특정 도구 호출 시퀀스를 따랐는지 평가하는 데 이상적입니다. 이는 에이전트 추론 프로세스를 검증하고, 도구 사용 오류를 디버깅하고, 규정된 워크플로 준수를 보장하는 데 유용하며, 특히 여러 도구 사용 경로가 유사한 최종 답변으로 이어질 수 있지만 하나의 경로만 올바른 것으로 간주되는 경우에 유용합니다.

### 세부 정보

이 기준은 루브릭으로 정의한 특정 규칙에 따라 도구 사용을 평가하는 유연한 방법을 제공합니다. 예를 들어 특정 도구가 호출되었는지, 매개변수가 올바른지 또는 도구가 특정 순서로 호출되었는지 확인하는 루브릭을 정의할 수 있습니다.

이 기준은 LLM 판사를 사용하여 각 루브릭에 대해 에이전트의 도구 호출 및 응답을 평가하고 각각에 대해 `예`(1.0) 또는 `아니요`(0.0) 판정을 내립니다. 다른 LLM 기반 메트릭과 마찬가지로 호출당 판사 모델을 여러 번 샘플링하고 다수결 투표를 사용하여 해당 호출의 각 루브릭에 대한 점수를 결정합니다. 호출의 전체 점수는 루브릭 점수의 평균입니다. 평가 사례의 최종 기준 점수는 모든 호출에 대한 이러한 전체 점수의 평균입니다.

### 이 기준을 사용하는 방법

이 기준은 `RubricsBasedCriterion`을 사용하며, `EvalConfig`에 루브릭 목록을 제공해야 합니다. 각 루브릭은 고유 ID와 해당 내용으로 정의되어야 하며, 평가할 도구 사용의 특정 측면을 설명해야 합니다.

`EvalConfig` 항목 예:

```json
{
  "criteria": {
    "rubric_based_tool_use_quality_v1": {
      "threshold": 1.0,
      "judge_model_options": {
        "judge_model": "gemini-2.5-flash",
        "num_samples": 5
      },
      "rubrics": [
        {
          "rubric_id": "geocoding_called",
          "rubric_content": {
            "text_property": "에이전트는 GetWeather 도구를 호출하기 전에 GeoCoding 도구를 호출합니다."
          }
        },
        {
          "rubric_id": "getweather_called",
          "rubric_content": {
            "text_property": "에이전트는 사용자의 위치에서 파생된 좌표로 GetWeather 도구를 호출합니다."
          }
        }
      ]
    }
  }
}
```

### 출력 및 해석 방법

이 기준은 0.0에서 1.0 사이의 전체 점수를 출력하며, 1.0은 에이전트의 도구 사용이 모든 호출에서 모든 루브릭을 충족했음을 나타내고 0.0은 충족된 루브릭이 없음을 나타냅니다. 결과에는 각 호출에 대한 자세한 루브릭별 점수도 포함됩니다. 값이 높을수록 좋습니다.

## hallucinations_v1

이 기준은 모델 응답에 거짓, 모순 또는 지원되지 않는 주장이 포함되어 있는지 평가합니다.

### 이 기준을 사용하는 경우

에이전트의 응답이 제공된 컨텍스트(예: 도구 출력, 사용자 쿼리, 지침)에 근거하고 환각을 포함하지 않도록 하려면 이 기준을 사용합니다.

### 세부 정보

이 기준은 개발자 지침, 사용자 프롬프트, 도구 정의, 도구 호출 및 그 결과를 포함하는 컨텍스트를 기반으로 모델 응답에 거짓, 모순 또는 지원되지 않는 주장이 포함되어 있는지 평가합니다. LLM 판사를 사용하며 다음 두 단계 프로세스를 따릅니다.

1.  **세그먼터**: 에이전트 응답을 개별 문장으로 분할합니다.
2.  **문장 유효성 검사기**: 제공된 컨텍스트에 대해 각 분할된 문장의 근거를 평가합니다. 각 문장은 `supported`, `unsupported`, `contradictory`, `disputed` 또는 `not_applicable`으로 레이블이 지정됩니다.

메트릭은 정확도 점수(`supported` 또는 `not_applicable`인 문장의 백분율)를 계산합니다. 기본적으로 최종 응답만 평가됩니다. 기준에서 `evaluate_intermediate_nl_responses`가 true로 설정된 경우 에이전트의 중간 자연어 응답도 평가됩니다.

### 이 기준을 사용하는 방법

이 기준은 `HallucinationsCriterion`을 사용하므로 평가 임계값, 판사 모델, 호출당 샘플 수 및 중간 자연어 응답 평가 여부를 구성할 수 있습니다.

`EvalConfig` 항목 예:

```json
{
  "criteria": {
    "hallucinations_v1": {
      "threshold": 0.8,
      "judge_model_options": {
            "judge_model": "gemini-2.5-flash",
          },
      "evaluate_intermediate_nl_responses": true
    }
  }
}

```

### 출력 및 해석 방법

이 기준은 0.0에서 1.0 사이의 점수를 반환합니다. 1.0의 점수는 에이전트 응답의 모든 문장이 컨텍스트에 근거함을 의미하며, 0.0에 가까운 점수는 많은 문장이 거짓, 모순 또는 지원되지 않음을 나타냅니다. 값이 높을수록 좋습니다.

## safety_v1

이 기준은 에이전트 응답의 안전성(무해성)을 평가합니다.

### 이 기준을 사용하는 경우

에이전트 응답이 안전 지침을 준수하고 유해하거나 부적절한 콘텐츠를 생성하지 않도록 해야 하는 경우 이 기준을 사용해야 합니다. 사용자 대면 애플리케이션이나 응답 안전성이 우선시되는 모든 시스템에 필수적입니다.

### 세부 정보

이 기준은 에이전트의 응답에 증오심 표현, 괴롭힘 또는 위험한 정보와 같은 유해한 콘텐츠가 포함되어 있는지 평가합니다. ADK 내에서 기본적으로 구현된 다른 메트릭과 달리 `safety_v1`은 평가를 Vertex AI General AI Eval SDK에 위임합니다.

### 이 기준을 사용하는 방법

이 기준을 사용하려면 Google Cloud 프로젝트가 필요합니다. Vertex AI SDK가 올바르게 작동하려면 일반적으로 에이전트 디렉터리의 `.env` 파일에 `GOOGLE_CLOUD_PROJECT` 및 `GOOGLE_CLOUD_LOCATION` 환경 변수가 설정되어 있어야 합니다.

`EvalConfig`의 `criteria` 사전에서 이 기준에 대한 임계값을 지정할 수 있습니다. 값은 0.0에서 1.0 사이의 부동 소수점이어야 하며, 응답이 통과로 간주되기 위한 최소 안전 점수를 나타냅니다.

`EvalConfig` 항목 예:

```json
{
  "criteria": {
    "safety_v1": 0.8
  }
}
```

### 출력 및 해석 방법

이 기준은 0.0에서 1.0 사이의 점수를 반환합니다. 1.0에 가까운 점수는 응답이 안전함을 나타내고 0.0에 가까운 점수는 잠재적인 안전 문제를 나타냅니다.
